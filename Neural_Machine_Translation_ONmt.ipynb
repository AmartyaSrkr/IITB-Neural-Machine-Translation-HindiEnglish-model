{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iBcy0C1xlrSv",
        "outputId": "42e12065-8924-42d3-a427-738e7fb867e9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/nmt_hien\n",
            "Cloning into 'MT-Preparation'...\n",
            "remote: Enumerating objects: 305, done.\u001b[K\n",
            "remote: Counting objects: 100% (131/131), done.\u001b[K\n",
            "remote: Compressing objects: 100% (69/69), done.\u001b[K\n",
            "remote: Total 305 (delta 66), reused 114 (delta 58), pack-reused 174 (from 1)\u001b[K\n",
            "Receiving objects: 100% (305/305), 84.51 KiB | 3.67 MiB/s, done.\n",
            "Resolving deltas: 100% (149/149), done.\n"
          ]
        }
      ],
      "source": [
        "# Creating a directory and cloning the MT-Preparation tool for preprocessing\n",
        "!mkdir nmt_hien\n",
        "%cd nmt_hien\n",
        "!git clone https://github.com/ymoslem/MT-Preparation.git"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AjoXvnxVmfXr",
        "outputId": "8d96a9f6-a2c4-452a-b10b-75fc365df40c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from -r MT-Preparation/requirements.txt (line 1)) (1.26.4)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from -r MT-Preparation/requirements.txt (line 2)) (2.1.4)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.10/dist-packages (from -r MT-Preparation/requirements.txt (line 3)) (0.2.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->-r MT-Preparation/requirements.txt (line 2)) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->-r MT-Preparation/requirements.txt (line 2)) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas->-r MT-Preparation/requirements.txt (line 2)) (2024.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->-r MT-Preparation/requirements.txt (line 2)) (1.16.0)\n"
          ]
        }
      ],
      "source": [
        "# Installing the requirments for specific versions of libraries\n",
        "!pip3 install -r MT-Preparation/requirements.txt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wUNA8d5tm4UW",
        "outputId": "e425c6ff-23fd-48cc-d89d-1f5cbf0f24a0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2024-09-30 03:22:58--  https://object.pouta.csc.fi/OPUS-pmindia/v1/moses/en-hi.txt.zip\n",
            "Resolving object.pouta.csc.fi (object.pouta.csc.fi)... 86.50.254.18, 86.50.254.19\n",
            "Connecting to object.pouta.csc.fi (object.pouta.csc.fi)|86.50.254.18|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 5811103 (5.5M) [application/zip]\n",
            "Saving to: ‘en-hi.txt.zip’\n",
            "\n",
            "en-hi.txt.zip       100%[===================>]   5.54M  3.25MB/s    in 1.7s    \n",
            "\n",
            "2024-09-30 03:23:01 (3.25 MB/s) - ‘en-hi.txt.zip’ saved [5811103/5811103]\n",
            "\n",
            "Archive:  en-hi.txt.zip\n",
            "  inflating: README                  \n",
            "  inflating: LICENSE                 \n",
            "  inflating: pmindia.en-hi.en        \n",
            "  inflating: pmindia.en-hi.hi        \n",
            "  inflating: pmindia.en-hi.xml       \n"
          ]
        }
      ],
      "source": [
        "# Getting the datasets from the https://opus.nlpl.eu/\n",
        "# Downloading and unzipping the dataset\n",
        "!wget https://object.pouta.csc.fi/OPUS-pmindia/v1/moses/en-hi.txt.zip\n",
        "# The above file is from IIT Bombay collection in OPUS\n",
        "\n",
        "!unzip en-hi.txt.zip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CQZAvUZzngEg",
        "outputId": "40ba3688-fe32-404f-b727-95b737b6a29e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataframe shape (rows, columns): (56832, 2)\n",
            "--- Rows with Empty Cells Deleted\t--> Rows: 56831\n",
            "--- Duplicates Deleted\t\t\t--> Rows: 56831\n",
            "--- Source-Copied Rows Deleted\t\t--> Rows: 56831\n",
            "--- Too Long Source/Target Deleted\t--> Rows: 56507\n",
            "--- HTML Removed\t\t\t--> Rows: 56507\n",
            "--- Rows will remain true-cased\t\t--> Rows: 56507\n",
            "--- Rows with Empty Cells Deleted\t--> Rows: 56507\n",
            "--- Rows Shuffled\t\t\t--> Rows: 56507\n",
            "--- Source Saved: pmindia.en-hi.hi-filtered.hi\n",
            "--- Target Saved: pmindia.en-hi.en-filtered.en\n"
          ]
        }
      ],
      "source": [
        "# Data Filtering\n",
        "# Filtering out low-quality segments for improving quality of MT model\n",
        "\n",
        "!python3 MT-Preparation/filtering/filter.py pmindia.en-hi.hi pmindia.en-hi.en hi en"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aJUwdhQwoFxn",
        "outputId": "e7924004-c010-4a33-ad1c-4ac45f9519b2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "extra  filtering  README.md  requirements.txt  subwording  train_dev_split\n"
          ]
        }
      ],
      "source": [
        "!ls MT-Preparation/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jFcLwzJHoOHI",
        "outputId": "14075e86-9a0f-4095-e02a-dadeb41f9cf6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1-train_bpe.py\t\t  1-train_unigram.py  3-desubword.py\n",
            "1-train_unigram_joint.py  2-subword.py\t      spm_to_vocab.py\n"
          ]
        }
      ],
      "source": [
        "!ls MT-Preparation/subwording/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nxwgXA82oTMH",
        "outputId": "e6bd92e8-6110-454d-e6d7-555aaed86294"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "sentencepiece_trainer.cc(178) LOG(INFO) Running command: --input=pmindia.en-hi.hi-filtered.hi --model_prefix=source --vocab_size=50000 --hard_vocab_limit=false --split_digits=true\n",
            "sentencepiece_trainer.cc(78) LOG(INFO) Starts training with : \n",
            "trainer_spec {\n",
            "  input: pmindia.en-hi.hi-filtered.hi\n",
            "  input_format: \n",
            "  model_prefix: source\n",
            "  model_type: UNIGRAM\n",
            "  vocab_size: 50000\n",
            "  self_test_sample_size: 0\n",
            "  character_coverage: 0.9995\n",
            "  input_sentence_size: 0\n",
            "  shuffle_input_sentence: 1\n",
            "  seed_sentencepiece_size: 1000000\n",
            "  shrinking_factor: 0.75\n",
            "  max_sentence_length: 4192\n",
            "  num_threads: 16\n",
            "  num_sub_iterations: 2\n",
            "  max_sentencepiece_length: 16\n",
            "  split_by_unicode_script: 1\n",
            "  split_by_number: 1\n",
            "  split_by_whitespace: 1\n",
            "  split_digits: 1\n",
            "  pretokenization_delimiter: \n",
            "  treat_whitespace_as_suffix: 0\n",
            "  allow_whitespace_only_pieces: 0\n",
            "  required_chars: \n",
            "  byte_fallback: 0\n",
            "  vocabulary_output_piece_score: 1\n",
            "  train_extremely_large_corpus: 0\n",
            "  seed_sentencepieces_file: \n",
            "  hard_vocab_limit: 0\n",
            "  use_all_vocab: 0\n",
            "  unk_id: 0\n",
            "  bos_id: 1\n",
            "  eos_id: 2\n",
            "  pad_id: -1\n",
            "  unk_piece: <unk>\n",
            "  bos_piece: <s>\n",
            "  eos_piece: </s>\n",
            "  pad_piece: <pad>\n",
            "  unk_surface:  ⁇ \n",
            "  enable_differential_privacy: 0\n",
            "  differential_privacy_noise_level: 0\n",
            "  differential_privacy_clipping_threshold: 0\n",
            "}\n",
            "normalizer_spec {\n",
            "  name: nmt_nfkc\n",
            "  add_dummy_prefix: 1\n",
            "  remove_extra_whitespaces: 1\n",
            "  escape_whitespaces: 1\n",
            "  normalization_rule_tsv: \n",
            "}\n",
            "denormalizer_spec {}\n",
            "trainer_interface.cc(353) LOG(INFO) SentenceIterator is not specified. Using MultiFileSentenceIterator.\n",
            "trainer_interface.cc(185) LOG(INFO) Loading corpus: pmindia.en-hi.hi-filtered.hi\n",
            "trainer_interface.cc(409) LOG(INFO) Loaded all 56507 sentences\n",
            "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <unk>\n",
            "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <s>\n",
            "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: </s>\n",
            "trainer_interface.cc(430) LOG(INFO) Normalizing sentences...\n",
            "trainer_interface.cc(539) LOG(INFO) all chars count=6357595\n",
            "trainer_interface.cc(550) LOG(INFO) Done: 99.9536% characters are covered.\n",
            "trainer_interface.cc(560) LOG(INFO) Alphabet size=119\n",
            "trainer_interface.cc(561) LOG(INFO) Final character coverage=0.999536\n",
            "trainer_interface.cc(592) LOG(INFO) Done! preprocessed 56507 sentences.\n",
            "unigram_model_trainer.cc(265) LOG(INFO) Making suffix array...\n",
            "unigram_model_trainer.cc(269) LOG(INFO) Extracting frequent sub strings... node_num=3333980\n",
            "unigram_model_trainer.cc(312) LOG(INFO) Initialized 78481 seed sentencepieces\n",
            "trainer_interface.cc(598) LOG(INFO) Tokenizing input sentences with whitespace: 56507\n",
            "trainer_interface.cc(609) LOG(INFO) Done! 51824\n",
            "unigram_model_trainer.cc(602) LOG(INFO) Using 51824 sentences for EM training\n",
            "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=32110 obj=10.8636 num_tokens=122090 num_tokens/piece=3.80224\n",
            "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=26958 obj=8.6797 num_tokens=122215 num_tokens/piece=4.53353\n",
            "trainer_interface.cc(687) LOG(INFO) Saving model: source.model\n",
            "trainer_interface.cc(699) LOG(INFO) Saving vocabs: source.vocab\n",
            "Done, training a SentencepPiece model for the Source finished successfully!\n",
            "sentencepiece_trainer.cc(178) LOG(INFO) Running command: --input=pmindia.en-hi.en-filtered.en --model_prefix=target --vocab_size=50000 --hard_vocab_limit=false --split_digits=true\n",
            "sentencepiece_trainer.cc(78) LOG(INFO) Starts training with : \n",
            "trainer_spec {\n",
            "  input: pmindia.en-hi.en-filtered.en\n",
            "  input_format: \n",
            "  model_prefix: target\n",
            "  model_type: UNIGRAM\n",
            "  vocab_size: 50000\n",
            "  self_test_sample_size: 0\n",
            "  character_coverage: 0.9995\n",
            "  input_sentence_size: 0\n",
            "  shuffle_input_sentence: 1\n",
            "  seed_sentencepiece_size: 1000000\n",
            "  shrinking_factor: 0.75\n",
            "  max_sentence_length: 4192\n",
            "  num_threads: 16\n",
            "  num_sub_iterations: 2\n",
            "  max_sentencepiece_length: 16\n",
            "  split_by_unicode_script: 1\n",
            "  split_by_number: 1\n",
            "  split_by_whitespace: 1\n",
            "  split_digits: 1\n",
            "  pretokenization_delimiter: \n",
            "  treat_whitespace_as_suffix: 0\n",
            "  allow_whitespace_only_pieces: 0\n",
            "  required_chars: \n",
            "  byte_fallback: 0\n",
            "  vocabulary_output_piece_score: 1\n",
            "  train_extremely_large_corpus: 0\n",
            "  seed_sentencepieces_file: \n",
            "  hard_vocab_limit: 0\n",
            "  use_all_vocab: 0\n",
            "  unk_id: 0\n",
            "  bos_id: 1\n",
            "  eos_id: 2\n",
            "  pad_id: -1\n",
            "  unk_piece: <unk>\n",
            "  bos_piece: <s>\n",
            "  eos_piece: </s>\n",
            "  pad_piece: <pad>\n",
            "  unk_surface:  ⁇ \n",
            "  enable_differential_privacy: 0\n",
            "  differential_privacy_noise_level: 0\n",
            "  differential_privacy_clipping_threshold: 0\n",
            "}\n",
            "normalizer_spec {\n",
            "  name: nmt_nfkc\n",
            "  add_dummy_prefix: 1\n",
            "  remove_extra_whitespaces: 1\n",
            "  escape_whitespaces: 1\n",
            "  normalization_rule_tsv: \n",
            "}\n",
            "denormalizer_spec {}\n",
            "trainer_interface.cc(353) LOG(INFO) SentenceIterator is not specified. Using MultiFileSentenceIterator.\n",
            "trainer_interface.cc(185) LOG(INFO) Loading corpus: pmindia.en-hi.en-filtered.en\n",
            "trainer_interface.cc(409) LOG(INFO) Loaded all 56507 sentences\n",
            "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <unk>\n",
            "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <s>\n",
            "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: </s>\n",
            "trainer_interface.cc(430) LOG(INFO) Normalizing sentences...\n",
            "trainer_interface.cc(539) LOG(INFO) all chars count=6634735\n",
            "trainer_interface.cc(550) LOG(INFO) Done: 99.9567% characters are covered.\n",
            "trainer_interface.cc(560) LOG(INFO) Alphabet size=76\n",
            "trainer_interface.cc(561) LOG(INFO) Final character coverage=0.999567\n",
            "trainer_interface.cc(592) LOG(INFO) Done! preprocessed 56507 sentences.\n",
            "unigram_model_trainer.cc(265) LOG(INFO) Making suffix array...\n",
            "unigram_model_trainer.cc(269) LOG(INFO) Extracting frequent sub strings... node_num=3554270\n",
            "unigram_model_trainer.cc(312) LOG(INFO) Initialized 82367 seed sentencepieces\n",
            "trainer_interface.cc(598) LOG(INFO) Tokenizing input sentences with whitespace: 56507\n",
            "trainer_interface.cc(609) LOG(INFO) Done! 58677\n",
            "unigram_model_trainer.cc(602) LOG(INFO) Using 58677 sentences for EM training\n",
            "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=31779 obj=11.1865 num_tokens=134677 num_tokens/piece=4.23792\n",
            "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=26633 obj=8.67555 num_tokens=135440 num_tokens/piece=5.08542\n",
            "trainer_interface.cc(687) LOG(INFO) Saving model: target.model\n",
            "trainer_interface.cc(699) LOG(INFO) Saving vocabs: target.vocab\n",
            "Done, training a SentencepPiece model for the Target finished successfully!\n"
          ]
        }
      ],
      "source": [
        "# Train a SentencePiece model for subword tokenization\n",
        "!python MT-Preparation/subwording/1-train_unigram.py pmindia.en-hi.hi-filtered.hi pmindia.en-hi.en-filtered.en"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IHMQh9HFosOB",
        "outputId": "93fe088d-0e32-40ee-8181-0974fa5ff94d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "en-hi.txt.zip\t  pmindia.en-hi.en-filtered.en\tREADME\t      target.vocab\n",
            "LICENSE\t\t  pmindia.en-hi.hi\t\tsource.model\n",
            "MT-Preparation\t  pmindia.en-hi.hi-filtered.hi\tsource.vocab\n",
            "pmindia.en-hi.en  pmindia.en-hi.xml\t\ttarget.model\n"
          ]
        }
      ],
      "source": [
        "!ls"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W2RE9_BdoyYg",
        "outputId": "c58eff21-c3e2-49ac-eda3-dc5185803288"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Source Model: source.model\n",
            "Target Model: target.model\n",
            "Source Dataset: pmindia.en-hi.hi-filtered.hi\n",
            "Target Dataset: pmindia.en-hi.en-filtered.en\n",
            "Done subwording the source file! Output: pmindia.en-hi.hi-filtered.hi.subword\n",
            "Done subwording the target file! Output: pmindia.en-hi.en-filtered.en.subword\n"
          ]
        }
      ],
      "source": [
        "# Subword the dataset\n",
        "!python3 MT-Preparation/subwording/2-subword.py source.model target.model  pmindia.en-hi.hi-filtered.hi pmindia.en-hi.en-filtered.en"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r3ohz5ODpPhp",
        "outputId": "f8cfc885-9ab1-4984-bf96-3e5ff86a0e99"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "en-hi.txt.zip\t\t      pmindia.en-hi.en-filtered.en.subword  README\n",
            "LICENSE\t\t\t      pmindia.en-hi.hi\t\t\t    source.model\n",
            "MT-Preparation\t\t      pmindia.en-hi.hi-filtered.hi\t    source.vocab\n",
            "pmindia.en-hi.en\t      pmindia.en-hi.hi-filtered.hi.subword  target.model\n",
            "pmindia.en-hi.en-filtered.en  pmindia.en-hi.xml\t\t\t    target.vocab\n"
          ]
        }
      ],
      "source": [
        "!ls"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "81Y2oEh3pT7e",
        "outputId": "4b3ad959-3a40-4f1b-ee6f-8af92d3764ee"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "इस समझौता ज्ञापन से व्‍यावसायिक शिक्षा तथा प्रशिक्षण और कौशल विकास के क्षेत्र में दोनों देशों के बीच द्विपक्षीय सहयोग प्रगाढ़ बनाने का मार्ग प्रशस्‍त होगा। \n",
            "प्रधानमंत्री ने टाटा मेमोरियल अस्पताल के जरिए कैंसर अनुसंधान तथा इलाज जैसे गंभीर क्षेत्र में परमाणु ऊर्जा विभाग के वैज्ञानिकों के योगदान की सराहना की। \n",
            "प्रधानमंत्री ने भारत में तेजी से हो रहे बदलावों के बारे में भी बातचीत की। \n",
            "-----\n",
            "The MoU would pave the way for closer bilateral cooperation between the two countries in the field of vocational education and training and skill development. \n",
            "Prime Minister lauded the contribution of DAE scientists in the critical area of cancer research and treatment through the Tata Memorial Hospital. \n",
            "The Prime Minister talked about the changes happening in India. \n"
          ]
        }
      ],
      "source": [
        "# First 3 lines before subwording\n",
        "!head -n 3  pmindia.en-hi.hi-filtered.hi && echo \"-----\" && head -n 3 pmindia.en-hi.en-filtered.en"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cfJzcxd4pnGm",
        "outputId": "e18bac8b-d2b8-4e9f-8e0c-39f93e35fd2c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "▁इस ▁समझौता ▁ज्ञापन ▁से ▁व्‍यावसायिक ▁शिक्षा ▁तथा ▁प्रशिक्षण ▁और ▁कौशल ▁विकास ▁के ▁क्षेत्र ▁में ▁दोनों ▁देशों ▁के ▁बीच ▁द्विपक्षीय ▁सहयोग ▁प्रगाढ़ ▁बनाने ▁का ▁मार्ग ▁प्र शस्‍त ▁होगा ।\n",
            "▁प्रधानमंत्री ▁ने ▁टाटा ▁मेमोर ियल ▁अस्पताल ▁के ▁जरिए ▁कैंसर ▁अनुसंधान ▁तथा ▁इलाज ▁जैसे ▁गंभीर ▁क्षेत्र ▁में ▁परमाणु ▁ऊर्जा ▁विभाग ▁के ▁वैज्ञानिकों ▁के ▁योगदान ▁की ▁सराहना ▁की ।\n",
            "▁प्रधानमंत्री ▁ने ▁भारत ▁में ▁तेजी ▁से ▁हो ▁रहे ▁बदलावों ▁के ▁बारे ▁में ▁भी ▁बातचीत ▁की ।\n",
            "-----\n",
            "▁The ▁MoU ▁would ▁pave ▁the ▁way ▁for ▁closer ▁bilateral ▁cooperation ▁between ▁the ▁two ▁countries ▁in ▁the ▁field ▁of ▁vocational ▁education ▁and ▁training ▁and ▁skill ▁development .\n",
            "▁Prim e ▁Minister ▁laud ed ▁the ▁contribution ▁of ▁DAE ▁scientists ▁in ▁the ▁critical ▁area ▁of ▁cancer ▁research ▁and ▁treatment ▁through ▁the ▁Tata ▁Memorial ▁Hospital .\n",
            "▁The ▁Prim e ▁Minister ▁talk ed ▁about ▁the ▁changes ▁happening ▁in ▁India .\n"
          ]
        }
      ],
      "source": [
        "# Now first 3 lines after subwording\n",
        "!head -n 3 pmindia.en-hi.hi-filtered.hi.subword && echo \"-----\" && head -n 3 pmindia.en-hi.en-filtered.en.subword"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZlhWnpx7p2we",
        "outputId": "73f0293b-cd24-4a68-ec8b-3daf87dfcb69"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train_dev_split.py  train_dev_test_split.py\n"
          ]
        }
      ],
      "source": [
        "# Data Splitting\n",
        "# Split the dataset into training set, development set and test set\n",
        "# The range of the dev set and the test set should be within 1000 and 5000 segments/sentence-pairs...\n",
        "!ls MT-Preparation/train_dev_split/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1nmCl6ueqbeR",
        "outputId": "243037f4-93cd-4bd9-bfc0-3d9d97677b8a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataframe shape: (56507, 2)\n",
            "--- Empty Cells Deleted --> Rows: 56507\n",
            "--- Wrote Files\n",
            "Done!\n",
            "Output files\n",
            "pmindia.en-hi.hi-filtered.hi.subword.train\n",
            "pmindia.en-hi.en-filtered.en.subword.train\n",
            "pmindia.en-hi.hi-filtered.hi.subword.dev\n",
            "pmindia.en-hi.en-filtered.en.subword.dev\n",
            "pmindia.en-hi.hi-filtered.hi.subword.test\n",
            "pmindia.en-hi.en-filtered.en.subword.test\n"
          ]
        }
      ],
      "source": [
        "!python MT-Preparation/train_dev_split/train_dev_test_split.py 2000 2000  pmindia.en-hi.hi-filtered.hi.subword pmindia.en-hi.en-filtered.en.subword"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BYFgmzJKqlzB",
        "outputId": "d53e6f5a-c6a1-4249-81cf-ab720e35f98c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    2000 pmindia.en-hi.en-filtered.en.subword.dev\n",
            "    2000 pmindia.en-hi.en-filtered.en.subword.test\n",
            "   52507 pmindia.en-hi.en-filtered.en.subword.train\n",
            "    2000 pmindia.en-hi.hi-filtered.hi.subword.dev\n",
            "    2000 pmindia.en-hi.hi-filtered.hi.subword.test\n",
            "   52507 pmindia.en-hi.hi-filtered.hi.subword.train\n",
            "  113014 total\n"
          ]
        }
      ],
      "source": [
        "# Line count\n",
        "!wc -l *.subword.*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3GIQ59ZFquHB",
        "outputId": "c7b3b2de-e244-47bd-b8bf-04653b57a939"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "My name is: Amartya Sarkar \n",
            "\n",
            "---First line---\n",
            "==> pmindia.en-hi.en-filtered.en.subword.train <==\n",
            "▁The ▁MoU ▁would ▁pave ▁the ▁way ▁for ▁closer ▁bilateral ▁cooperation ▁between ▁the ▁two ▁countries ▁in ▁the ▁field ▁of ▁vocational ▁education ▁and ▁training ▁and ▁skill ▁development .\n",
            "\n",
            "==> pmindia.en-hi.hi-filtered.hi.subword.train <==\n",
            "▁इस ▁समझौता ▁ज्ञापन ▁से ▁व्‍यावसायिक ▁शिक्षा ▁तथा ▁प्रशिक्षण ▁और ▁कौशल ▁विकास ▁के ▁क्षेत्र ▁में ▁दोनों ▁देशों ▁के ▁बीच ▁द्विपक्षीय ▁सहयोग ▁प्रगाढ़ ▁बनाने ▁का ▁मार्ग ▁प्र शस्‍त ▁होगा ।\n",
            "\n",
            "==> pmindia.en-hi.en-filtered.en.subword.dev <==\n",
            "▁It ▁is ▁expected ▁that ▁the ▁expanded ▁use ▁of ▁this ▁technology ▁in ▁these ▁categories ▁of ▁cases ▁would ▁result ▁not ▁only ▁in ▁speed ier ▁justice ▁delivery ▁but ▁also ▁in ▁increased ▁conviction ▁rates , ▁which ▁at ▁present ▁is ▁only ▁around ▁ 3 0 % ▁( NCRB ▁Statistics ▁for ▁ 2 0 1 6 ).\n",
            "\n",
            "==> pmindia.en-hi.hi-filtered.hi.subword.dev <==\n",
            "▁उम्मीद ▁है ▁कि ▁अपराधों ▁के ▁ऐसे ▁वर्गों ▁में ▁इस ▁प्रौद्योगिकी ▁के ▁विस्तारित ▁उपयोग ▁से ▁न ▁के वल ▁न्याय िक ▁प्रक्रिया ▁में ▁तेजी ▁आएगी , ▁बल्कि ▁सजा ▁दिलान े ▁की ▁दर ▁भी ▁बढ़ेगी , ▁जो ▁वर्तमान ▁में ▁के वल ▁ 3 0 ▁प्रतिशत ▁( 2 0 1 6 ▁के ▁ एनसीआरबी ▁आंकड़े ) ▁है ।\n",
            "\n",
            "==> pmindia.en-hi.en-filtered.en.subword.test <==\n",
            "▁The ▁Prim e ▁Minister ▁met ▁UN ▁Secretary ▁General ▁Ban ▁Ki ▁Moon .\n",
            "\n",
            "==> pmindia.en-hi.hi-filtered.hi.subword.test <==\n",
            "▁प्रधानमंत्री ▁ने ▁संयुक्त ▁राष्ट्र ▁के ▁महासचिव ▁श्री ▁बान ▁की ▁मून ▁से ▁भेंट ▁की ।\n",
            "\n",
            "---Last line---\n",
            "==> pmindia.en-hi.en-filtered.en.subword.train <==\n",
            "▁“ Union ▁Budget ▁ 2 0 1 5 ▁is ▁a ▁Budget ▁with ▁a ▁clear ▁vision .\n",
            "\n",
            "==> pmindia.en-hi.hi-filtered.hi.subword.train <==\n",
            "▁‘ केंद्रीय ▁बजट ▁ 2 0 1 5 ▁में ▁विजन ▁स्पष्ट ▁है ।\n",
            "\n",
            "==> pmindia.en-hi.en-filtered.en.subword.dev <==\n",
            "▁I ▁convey ed ▁our ▁sincere ▁support ▁and ▁good ▁wishes ▁for ▁Sri ▁Lanka ` s ▁new ▁journey ▁of ▁peace , ▁reconciliation ▁and ▁progress .\n",
            "\n",
            "==> pmindia.en-hi.hi-filtered.hi.subword.dev <==\n",
            "▁मैं , ▁श्रीलंका ▁में ▁शांति , ▁सामंजस्‍य ▁और ▁प्रगति ▁की ▁नई ▁यात्रा ▁के ▁लिए ▁समर्थन ▁और ▁शुभकामनाएं ▁देता ▁हूं ।\n",
            "\n",
            "==> pmindia.en-hi.en-filtered.en.subword.test <==\n",
            "▁The ▁approval ▁seek s ▁to ▁ensure ▁that ▁the ▁doctors ▁belonging ▁to ▁Central ▁Health ▁Service ▁( CHS ) ▁and ▁of ▁other ▁Ministries ▁/ ▁Departments ▁/ ▁entities ▁of ▁the ▁Central ▁Government , ▁after ▁attain ing ▁the ▁age ▁of ▁ 6 2 ▁years , ▁work ▁exclusive ly ▁in ▁their ▁respective ▁fields ▁of ▁clinical ▁expertise .\n",
            "\n",
            "==> pmindia.en-hi.hi-filtered.hi.subword.test <==\n",
            "▁इस ▁स्‍वीकृति ▁का ▁उद्देश्‍य ▁यह ▁सुनिश्चित ▁करना ▁है ▁कि ▁केंद्रीय ▁स्‍वास्‍थ्‍य ▁सेवा ▁( सीएचएस ) ▁तथा ▁अन्‍य ▁मंत्रालयों / विभागों / केंद्र ▁सरकार ▁की ▁संस्‍थाओं ▁के ▁डॉक्‍टर ▁ 6 2 ▁वर्ष ▁की ▁उम्र ▁पूर ी ▁होने ▁पर ▁विशेषज्ञता ▁वाले ▁क्षेत्रों ▁में ▁विशेष ▁रूप ▁से ▁कार्य ▁करें ।\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# -------------------------------------------\n",
        "!echo -e \"My name is: Amartya Sarkar \\n\"\n",
        "# -------------------------------------------\n",
        "!echo \"---First line---\"\n",
        "!head -n 1 *.{train,dev,test}\n",
        "\n",
        "!echo -e \"\\n---Last line---\"\n",
        "!tail -n 1 *.{train,dev,test}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eLbpdffwq7mJ",
        "outputId": "f2200e79-e9f7-43c7-da30-dc8d7b72a339"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting OpenNMT-py\n",
            "  Downloading OpenNMT_py-3.5.1-py3-none-any.whl.metadata (8.8 kB)\n",
            "Collecting torch<2.3,>=2.1 (from OpenNMT-py)\n",
            "  Downloading torch-2.2.2-cp310-cp310-manylinux1_x86_64.whl.metadata (26 kB)\n",
            "Collecting configargparse (from OpenNMT-py)\n",
            "  Downloading ConfigArgParse-1.7-py3-none-any.whl.metadata (23 kB)\n",
            "Collecting ctranslate2<5,>=4 (from OpenNMT-py)\n",
            "  Downloading ctranslate2-4.4.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (10 kB)\n",
            "Requirement already satisfied: tensorboard>=2.3 in /usr/local/lib/python3.10/dist-packages (from OpenNMT-py) (2.17.0)\n",
            "Requirement already satisfied: flask in /usr/local/lib/python3.10/dist-packages (from OpenNMT-py) (2.2.5)\n",
            "Collecting waitress (from OpenNMT-py)\n",
            "  Downloading waitress-3.0.0-py3-none-any.whl.metadata (4.2 kB)\n",
            "Collecting pyonmttok<2,>=1.37 (from OpenNMT-py)\n",
            "  Downloading pyonmttok-1.37.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from OpenNMT-py) (6.0.2)\n",
            "Collecting sacrebleu (from OpenNMT-py)\n",
            "  Downloading sacrebleu-2.4.3-py3-none-any.whl.metadata (51 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m51.8/51.8 kB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting rapidfuzz (from OpenNMT-py)\n",
            "  Downloading rapidfuzz-3.10.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (11 kB)\n",
            "Collecting pyahocorasick (from OpenNMT-py)\n",
            "  Downloading pyahocorasick-2.1.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl.metadata (13 kB)\n",
            "Collecting fasttext-wheel (from OpenNMT-py)\n",
            "  Downloading fasttext_wheel-0.9.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (16 kB)\n",
            "Requirement already satisfied: spacy in /usr/local/lib/python3.10/dist-packages (from OpenNMT-py) (3.7.6)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from OpenNMT-py) (1.16.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from ctranslate2<5,>=4->OpenNMT-py) (71.0.4)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from ctranslate2<5,>=4->OpenNMT-py) (1.26.4)\n",
            "Requirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.10/dist-packages (from tensorboard>=2.3->OpenNMT-py) (1.4.0)\n",
            "Requirement already satisfied: grpcio>=1.48.2 in /usr/local/lib/python3.10/dist-packages (from tensorboard>=2.3->OpenNMT-py) (1.64.1)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard>=2.3->OpenNMT-py) (3.7)\n",
            "Requirement already satisfied: protobuf!=4.24.0,<5.0.0,>=3.19.6 in /usr/local/lib/python3.10/dist-packages (from tensorboard>=2.3->OpenNMT-py) (3.20.3)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard>=2.3->OpenNMT-py) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard>=2.3->OpenNMT-py) (3.0.4)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch<2.3,>=2.1->OpenNMT-py) (3.16.1)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch<2.3,>=2.1->OpenNMT-py) (4.12.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch<2.3,>=2.1->OpenNMT-py) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch<2.3,>=2.1->OpenNMT-py) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch<2.3,>=2.1->OpenNMT-py) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch<2.3,>=2.1->OpenNMT-py) (2024.6.1)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch<2.3,>=2.1->OpenNMT-py)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.1.105 (from torch<2.3,>=2.1->OpenNMT-py)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.1.105 (from torch<2.3,>=2.1->OpenNMT-py)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==8.9.2.26 (from torch<2.3,>=2.1->OpenNMT-py)\n",
            "  Downloading nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.1.3.1 (from torch<2.3,>=2.1->OpenNMT-py)\n",
            "  Downloading nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.0.2.54 (from torch<2.3,>=2.1->OpenNMT-py)\n",
            "  Downloading nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.2.106 (from torch<2.3,>=2.1->OpenNMT-py)\n",
            "  Downloading nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.4.5.107 (from torch<2.3,>=2.1->OpenNMT-py)\n",
            "  Downloading nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.1.0.106 (from torch<2.3,>=2.1->OpenNMT-py)\n",
            "  Downloading nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-nccl-cu12==2.19.3 (from torch<2.3,>=2.1->OpenNMT-py)\n",
            "  Downloading nvidia_nccl_cu12-2.19.3-py3-none-manylinux1_x86_64.whl.metadata (1.8 kB)\n",
            "Collecting nvidia-nvtx-cu12==12.1.105 (from torch<2.3,>=2.1->OpenNMT-py)\n",
            "  Downloading nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.7 kB)\n",
            "Collecting triton==2.2.0 (from torch<2.3,>=2.1->OpenNMT-py)\n",
            "  Downloading triton-2.2.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.4 kB)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.10/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch<2.3,>=2.1->OpenNMT-py) (12.6.68)\n",
            "Collecting pybind11>=2.2 (from fasttext-wheel->OpenNMT-py)\n",
            "  Downloading pybind11-2.13.6-py3-none-any.whl.metadata (9.5 kB)\n",
            "Requirement already satisfied: itsdangerous>=2.0 in /usr/local/lib/python3.10/dist-packages (from flask->OpenNMT-py) (2.2.0)\n",
            "Requirement already satisfied: click>=8.0 in /usr/local/lib/python3.10/dist-packages (from flask->OpenNMT-py) (8.1.7)\n",
            "Collecting portalocker (from sacrebleu->OpenNMT-py)\n",
            "  Downloading portalocker-2.10.1-py3-none-any.whl.metadata (8.5 kB)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.10/dist-packages (from sacrebleu->OpenNMT-py) (2024.9.11)\n",
            "Requirement already satisfied: tabulate>=0.8.9 in /usr/local/lib/python3.10/dist-packages (from sacrebleu->OpenNMT-py) (0.9.0)\n",
            "Collecting colorama (from sacrebleu->OpenNMT-py)\n",
            "  Downloading colorama-0.4.6-py2.py3-none-any.whl.metadata (17 kB)\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.10/dist-packages (from sacrebleu->OpenNMT-py) (4.9.4)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.10/dist-packages (from spacy->OpenNMT-py) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from spacy->OpenNMT-py) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.10/dist-packages (from spacy->OpenNMT-py) (1.0.10)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy->OpenNMT-py) (2.0.8)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy->OpenNMT-py) (3.0.9)\n",
            "Requirement already satisfied: thinc<8.3.0,>=8.2.2 in /usr/local/lib/python3.10/dist-packages (from spacy->OpenNMT-py) (8.2.5)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.10/dist-packages (from spacy->OpenNMT-py) (1.1.3)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.10/dist-packages (from spacy->OpenNMT-py) (2.4.8)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.10/dist-packages (from spacy->OpenNMT-py) (2.0.10)\n",
            "Requirement already satisfied: weasel<0.5.0,>=0.1.0 in /usr/local/lib/python3.10/dist-packages (from spacy->OpenNMT-py) (0.4.1)\n",
            "Requirement already satisfied: typer<1.0.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from spacy->OpenNMT-py) (0.12.5)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.10/dist-packages (from spacy->OpenNMT-py) (4.66.5)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from spacy->OpenNMT-py) (2.32.3)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.10/dist-packages (from spacy->OpenNMT-py) (2.9.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from spacy->OpenNMT-py) (24.1)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from spacy->OpenNMT-py) (3.4.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch<2.3,>=2.1->OpenNMT-py) (2.1.5)\n",
            "Requirement already satisfied: language-data>=1.2 in /usr/local/lib/python3.10/dist-packages (from langcodes<4.0.0,>=3.2.0->spacy->OpenNMT-py) (1.2.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy->OpenNMT-py) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.23.4 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy->OpenNMT-py) (2.23.4)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy->OpenNMT-py) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy->OpenNMT-py) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy->OpenNMT-py) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy->OpenNMT-py) (2024.8.30)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.10/dist-packages (from thinc<8.3.0,>=8.2.2->spacy->OpenNMT-py) (0.7.11)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from thinc<8.3.0,>=8.2.2->spacy->OpenNMT-py) (0.1.5)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0.0,>=0.3.0->spacy->OpenNMT-py) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0.0,>=0.3.0->spacy->OpenNMT-py) (13.8.1)\n",
            "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from weasel<0.5.0,>=0.1.0->spacy->OpenNMT-py) (0.19.0)\n",
            "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in /usr/local/lib/python3.10/dist-packages (from weasel<0.5.0,>=0.1.0->spacy->OpenNMT-py) (7.0.4)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch<2.3,>=2.1->OpenNMT-py) (1.3.0)\n",
            "Requirement already satisfied: marisa-trie>=0.7.7 in /usr/local/lib/python3.10/dist-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy->OpenNMT-py) (1.2.0)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy->OpenNMT-py) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy->OpenNMT-py) (2.18.0)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.10/dist-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.1.0->spacy->OpenNMT-py) (1.16.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy->OpenNMT-py) (0.1.2)\n",
            "Downloading OpenNMT_py-3.5.1-py3-none-any.whl (262 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m262.8/262.8 kB\u001b[0m \u001b[31m15.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading ctranslate2-4.4.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (37.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m37.2/37.2 MB\u001b[0m \u001b[31m53.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pyonmttok-1.37.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (17.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m17.0/17.0 MB\u001b[0m \u001b[31m90.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading torch-2.2.2-cp310-cp310-manylinux1_x86_64.whl (755.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m755.5/755.5 MB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m410.6/410.6 MB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.1/14.1 MB\u001b[0m \u001b[31m103.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m23.7/23.7 MB\u001b[0m \u001b[31m87.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m823.6/823.6 kB\u001b[0m \u001b[31m52.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m731.7/731.7 MB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m121.6/121.6 MB\u001b[0m \u001b[31m7.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.5/56.5 MB\u001b[0m \u001b[31m12.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m124.2/124.2 MB\u001b[0m \u001b[31m7.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m196.0/196.0 MB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nccl_cu12-2.19.3-py3-none-manylinux1_x86_64.whl (166.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m166.0/166.0 MB\u001b[0m \u001b[31m7.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m99.1/99.1 kB\u001b[0m \u001b[31m8.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading triton-2.2.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (167.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m167.9/167.9 MB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading ConfigArgParse-1.7-py3-none-any.whl (25 kB)\n",
            "Downloading fasttext_wheel-0.9.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.4/4.4 MB\u001b[0m \u001b[31m98.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pyahocorasick-2.1.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (110 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m110.7/110.7 kB\u001b[0m \u001b[31m10.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading rapidfuzz-3.10.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m86.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading sacrebleu-2.4.3-py3-none-any.whl (103 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m104.0/104.0 kB\u001b[0m \u001b[31m10.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading waitress-3.0.0-py3-none-any.whl (56 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.7/56.7 kB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pybind11-2.13.6-py3-none-any.whl (243 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m243.3/243.3 kB\u001b[0m \u001b[31m23.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading colorama-0.4.6-py2.py3-none-any.whl (25 kB)\n",
            "Downloading portalocker-2.10.1-py3-none-any.whl (18 kB)\n",
            "Installing collected packages: waitress, triton, rapidfuzz, pyonmttok, pybind11, pyahocorasick, portalocker, nvidia-nvtx-cu12, nvidia-nccl-cu12, nvidia-cusparse-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, ctranslate2, configargparse, colorama, sacrebleu, nvidia-cusolver-cu12, nvidia-cudnn-cu12, fasttext-wheel, torch, OpenNMT-py\n",
            "  Attempting uninstall: nvidia-nccl-cu12\n",
            "    Found existing installation: nvidia-nccl-cu12 2.23.4\n",
            "    Uninstalling nvidia-nccl-cu12-2.23.4:\n",
            "      Successfully uninstalled nvidia-nccl-cu12-2.23.4\n",
            "  Attempting uninstall: nvidia-cusparse-cu12\n",
            "    Found existing installation: nvidia-cusparse-cu12 12.5.3.3\n",
            "    Uninstalling nvidia-cusparse-cu12-12.5.3.3:\n",
            "      Successfully uninstalled nvidia-cusparse-cu12-12.5.3.3\n",
            "  Attempting uninstall: nvidia-cufft-cu12\n",
            "    Found existing installation: nvidia-cufft-cu12 11.2.6.59\n",
            "    Uninstalling nvidia-cufft-cu12-11.2.6.59:\n",
            "      Successfully uninstalled nvidia-cufft-cu12-11.2.6.59\n",
            "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
            "    Found existing installation: nvidia-cuda-runtime-cu12 12.6.68\n",
            "    Uninstalling nvidia-cuda-runtime-cu12-12.6.68:\n",
            "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.6.68\n",
            "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
            "    Found existing installation: nvidia-cuda-cupti-cu12 12.6.68\n",
            "    Uninstalling nvidia-cuda-cupti-cu12-12.6.68:\n",
            "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.6.68\n",
            "  Attempting uninstall: nvidia-cublas-cu12\n",
            "    Found existing installation: nvidia-cublas-cu12 12.6.1.4\n",
            "    Uninstalling nvidia-cublas-cu12-12.6.1.4:\n",
            "      Successfully uninstalled nvidia-cublas-cu12-12.6.1.4\n",
            "  Attempting uninstall: nvidia-cusolver-cu12\n",
            "    Found existing installation: nvidia-cusolver-cu12 11.6.4.69\n",
            "    Uninstalling nvidia-cusolver-cu12-11.6.4.69:\n",
            "      Successfully uninstalled nvidia-cusolver-cu12-11.6.4.69\n",
            "  Attempting uninstall: nvidia-cudnn-cu12\n",
            "    Found existing installation: nvidia-cudnn-cu12 9.4.0.58\n",
            "    Uninstalling nvidia-cudnn-cu12-9.4.0.58:\n",
            "      Successfully uninstalled nvidia-cudnn-cu12-9.4.0.58\n",
            "  Attempting uninstall: torch\n",
            "    Found existing installation: torch 2.4.1+cu121\n",
            "    Uninstalling torch-2.4.1+cu121:\n",
            "      Successfully uninstalled torch-2.4.1+cu121\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "torchaudio 2.4.1+cu121 requires torch==2.4.1, but you have torch 2.2.2 which is incompatible.\n",
            "torchvision 0.19.1+cu121 requires torch==2.4.1, but you have torch 2.2.2 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed OpenNMT-py-3.5.1 colorama-0.4.6 configargparse-1.7 ctranslate2-4.4.0 fasttext-wheel-0.9.2 nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-8.9.2.26 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.19.3 nvidia-nvtx-cu12-12.1.105 portalocker-2.10.1 pyahocorasick-2.1.0 pybind11-2.13.6 pyonmttok-1.37.1 rapidfuzz-3.10.0 sacrebleu-2.4.3 torch-2.2.2 triton-2.2.0 waitress-3.0.0\n"
          ]
        }
      ],
      "source": [
        "!pip3 install OpenNMT-py"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "ojw5-EWPrMlI"
      },
      "outputs": [],
      "source": [
        "config = '''# config.yaml\n",
        "\n",
        "\n",
        "## Where the samples will be written\n",
        "save_data: run\n",
        "\n",
        "# Training files\n",
        "data:\n",
        "    corpus_1:\n",
        "        path_src: pmindia.en-hi.hi-filtered.hi.subword.train\n",
        "        path_tgt: pmindia.en-hi.en-filtered.en.subword.train\n",
        "        transforms: [filtertoolong]\n",
        "    valid:\n",
        "        path_src: pmindia.en-hi.hi-filtered.hi.subword.dev\n",
        "        path_tgt: pmindia.en-hi.en-filtered.en.subword.dev\n",
        "        transforms: [filtertoolong]\n",
        "\n",
        "# Vocabulary files, generated by onmt_build_vocab\n",
        "src_vocab: run/source.vocab\n",
        "tgt_vocab: run/target.vocab\n",
        "\n",
        "# Vocabulary size - should be the same as in sentence piece\n",
        "src_vocab_size: 50000\n",
        "tgt_vocab_size: 50000\n",
        "\n",
        "# Filter out source/target longer than n if [filtertoolong] enabled\n",
        "src_seq_length: 150\n",
        "src_seq_length: 150\n",
        "\n",
        "# Tokenization options\n",
        "src_subword_model: source.model\n",
        "tgt_subword_model: target.model\n",
        "\n",
        "# Where to save the log file and the output models/checkpoints\n",
        "log_file: train.log\n",
        "save_model: models/model.hien\n",
        "\n",
        "# Stop training if it does not imporve after n validations\n",
        "early_stopping: 4\n",
        "\n",
        "# Default: 5000 - Save a model checkpoint for each n\n",
        "save_checkpoint_steps: 1000\n",
        "\n",
        "# To save space, limit checkpoints to last n\n",
        "# keep_checkpoint: 3\n",
        "\n",
        "seed: 3435\n",
        "\n",
        "# Default: 100000 - Train the model to max n steps\n",
        "# Increase to 200000 or more for large datasets\n",
        "# For fine-tuning, add up the required steps to the original steps\n",
        "train_steps: 2000\n",
        "\n",
        "# Default: 10000 - Run validation after n steps\n",
        "valid_steps: 1000\n",
        "\n",
        "# Default: 4000 - for large datasets, try up to 8000\n",
        "warmup_steps: 1000\n",
        "report_every: 100\n",
        "\n",
        "# Number of GPUs, and IDs of GPUs\n",
        "world_size: 1\n",
        "gpu_ranks: [0]\n",
        "\n",
        "# Batching\n",
        "bucket_size: 262144\n",
        "num_workers: 0  # Default: 2, set to 0 when RAM out of memory\n",
        "batch_type: \"tokens\"\n",
        "batch_size: 4096   # Tokens per batch, change when CUDA out of memory\n",
        "valid_batch_size: 2048\n",
        "max_generator_batches: 2\n",
        "accum_count: [4]\n",
        "accum_steps: [0]\n",
        "\n",
        "# Optimization\n",
        "model_dtype: \"fp16\"\n",
        "optim: \"adam\"\n",
        "learning_rate: 2\n",
        "# warmup_steps: 8000\n",
        "decay_method: \"noam\"\n",
        "adam_beta2: 0.998\n",
        "max_grad_norm: 0\n",
        "label_smoothing: 0.1\n",
        "param_init: 0\n",
        "param_init_glorot: true\n",
        "normalization: \"tokens\"\n",
        "\n",
        "# Model\n",
        "encoder_type: transformer\n",
        "decoder_type: transformer\n",
        "position_encoding: true\n",
        "enc_layers: 6\n",
        "dec_layers: 6\n",
        "heads: 8\n",
        "hidden_size: 512\n",
        "word_vec_size: 512\n",
        "transformer_ff: 2048\n",
        "dropout_steps: [0]\n",
        "dropout: [0.1]\n",
        "attention_dropout: [0.1]\n",
        "'''\n",
        "\n",
        "with open(\"config.yaml\", \"w+\") as config_yaml:\n",
        "  config_yaml.write(config)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "deTl0PS5uDOG",
        "outputId": "72fff394-e64b-4d29-d878-3bd77b421e2b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "# config.yaml\n",
            "\n",
            "\n",
            "## Where the samples will be written\n",
            "save_data: run\n",
            "\n",
            "# Training files\n",
            "data:\n",
            "    corpus_1:\n",
            "        path_src: pmindia.en-hi.hi-filtered.hi.subword.train\n",
            "        path_tgt: pmindia.en-hi.en-filtered.en.subword.train\n",
            "        transforms: [filtertoolong]\n",
            "    valid:\n",
            "        path_src: pmindia.en-hi.hi-filtered.hi.subword.dev\n",
            "        path_tgt: pmindia.en-hi.en-filtered.en.subword.dev\n",
            "        transforms: [filtertoolong]\n",
            "\n",
            "# Vocabulary files, generated by onmt_build_vocab\n",
            "src_vocab: run/source.vocab\n",
            "tgt_vocab: run/target.vocab\n",
            "\n",
            "# Vocabulary size - should be the same as in sentence piece\n",
            "src_vocab_size: 50000\n",
            "tgt_vocab_size: 50000\n",
            "\n",
            "# Filter out source/target longer than n if [filtertoolong] enabled\n",
            "src_seq_length: 150\n",
            "src_seq_length: 150\n",
            "\n",
            "# Tokenization options\n",
            "src_subword_model: source.model\n",
            "tgt_subword_model: target.model\n",
            "\n",
            "# Where to save the log file and the output models/checkpoints\n",
            "log_file: train.log\n",
            "save_model: models/model.hien\n",
            "\n",
            "# Stop training if it does not imporve after n validations\n",
            "early_stopping: 4\n",
            "\n",
            "# Default: 5000 - Save a model checkpoint for each n\n",
            "save_checkpoint_steps: 1000\n",
            "\n",
            "# To save space, limit checkpoints to last n\n",
            "# keep_checkpoint: 3\n",
            "\n",
            "seed: 3435\n",
            "\n",
            "# Default: 100000 - Train the model to max n steps\n",
            "# Increase to 200000 or more for large datasets\n",
            "# For fine-tuning, add up the required steps to the original steps\n",
            "train_steps: 2000\n",
            "\n",
            "# Default: 10000 - Run validation after n steps\n",
            "valid_steps: 1000\n",
            "\n",
            "# Default: 4000 - for large datasets, try up to 8000\n",
            "warmup_steps: 1000\n",
            "report_every: 100\n",
            "\n",
            "# Number of GPUs, and IDs of GPUs\n",
            "world_size: 1\n",
            "gpu_ranks: [0]\n",
            "\n",
            "# Batching\n",
            "bucket_size: 262144\n",
            "num_workers: 0  # Default: 2, set to 0 when RAM out of memory\n",
            "batch_type: \"tokens\"\n",
            "batch_size: 4096   # Tokens per batch, change when CUDA out of memory\n",
            "valid_batch_size: 2048\n",
            "max_generator_batches: 2\n",
            "accum_count: [4]\n",
            "accum_steps: [0]\n",
            "\n",
            "# Optimization\n",
            "model_dtype: \"fp16\"\n",
            "optim: \"adam\"\n",
            "learning_rate: 2\n",
            "# warmup_steps: 8000\n",
            "decay_method: \"noam\"\n",
            "adam_beta2: 0.998\n",
            "max_grad_norm: 0\n",
            "label_smoothing: 0.1\n",
            "param_init: 0\n",
            "param_init_glorot: true\n",
            "normalization: \"tokens\"\n",
            "\n",
            "# Model\n",
            "encoder_type: transformer\n",
            "decoder_type: transformer\n",
            "position_encoding: true\n",
            "enc_layers: 6\n",
            "dec_layers: 6\n",
            "heads: 8\n",
            "hidden_size: 512\n",
            "word_vec_size: 512\n",
            "transformer_ff: 2048\n",
            "dropout_steps: [0]\n",
            "dropout: [0.1]\n",
            "attention_dropout: [0.1]\n"
          ]
        }
      ],
      "source": [
        "# Checking the content of the yaml file\n",
        "!cat config.yaml"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eSx6iBZvuXwp",
        "outputId": "72e44ca1-2dfb-47c3-c6dd-6c781157d3d7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2\n"
          ]
        }
      ],
      "source": [
        "!nproc --all"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aJaJjflrucVI",
        "outputId": "b0ac629d-fce6-4ccf-d500-7995740c30df"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Corpus corpus_1's weight should be given. We default it to 1 for you.\n",
            "[2024-09-30 03:26:56,834 INFO] Counter vocab from -1 samples.\n",
            "[2024-09-30 03:26:56,834 INFO] n_sample=-1: Build vocab on full datasets.\n",
            "[2024-09-30 03:26:59,356 INFO] * Transform statistics for corpus_1(50.00%):\n",
            "\t\t\t* FilterTooLongStats(filtered=2)\n",
            "\n",
            "[2024-09-30 03:26:59,444 INFO] Counters src: 24988\n",
            "[2024-09-30 03:26:59,444 INFO] Counters tgt: 24995\n"
          ]
        }
      ],
      "source": [
        "# Build the Vocabulary\n",
        "!onmt_build_vocab -config config.yaml -n_sample -1 -num_threads 2\n",
        "\n",
        "# -n_samples -1 takes all the segments for training\n",
        "# -num_threads changes to match the no.of CPUs to run it faster"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uJ0KZ9NwvB9x",
        "outputId": "dd052351-1f4e-49e8-87fd-9c28846f28cd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GPU 0: Tesla T4 (UUID: GPU-e32f7124-6bc7-71ce-38b7-f633b3d62d9a)\n"
          ]
        }
      ],
      "source": [
        "!nvidia-smi -L"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BrgDScEyT583",
        "outputId": "6bc8da0e-a180-418f-e937-adce4eb4e01f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "True\n",
            "Tesla T4\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "print(torch.cuda.is_available())\n",
        "print(torch.cuda.get_device_name(0))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ERXCU6JzesH3",
        "outputId": "15f799f9-5c83-49af-d161-208f5e90618c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2024-09-30 03:27:07,425 WARNING] Corpus corpus_1's weight should be given. We default it to 1 for you.\n",
            "[2024-09-30 03:27:07,425 INFO] Parsed 2 corpora from -data.\n",
            "[2024-09-30 03:27:07,425 INFO] Get special vocabs from Transforms: {'src': [], 'tgt': []}.\n",
            "[2024-09-30 03:27:07,534 INFO] The first 10 tokens of the vocabs are:['<unk>', '<blank>', '<s>', '</s>', '▁के', '।', '▁में', '▁की', ',', '▁और']\n",
            "[2024-09-30 03:27:07,535 INFO] The decoder start token is: <s>\n",
            "[2024-09-30 03:27:07,535 INFO] Building model...\n",
            "[2024-09-30 03:27:09,046 INFO] Switching model to float32 for amp/apex_amp\n",
            "[2024-09-30 03:27:09,047 INFO] Non quantized layer compute is fp16\n",
            "[2024-09-30 03:27:09,394 INFO] NMTModel(\n",
            "  (encoder): TransformerEncoder(\n",
            "    (embeddings): Embeddings(\n",
            "      (make_embedding): Sequential(\n",
            "        (emb_luts): Elementwise(\n",
            "          (0): Embedding(24992, 512, padding_idx=1)\n",
            "        )\n",
            "        (pe): PositionalEncoding()\n",
            "      )\n",
            "      (dropout): Dropout(p=0.1, inplace=False)\n",
            "    )\n",
            "    (transformer): ModuleList(\n",
            "      (0-5): 6 x TransformerEncoderLayer(\n",
            "        (self_attn): MultiHeadedAttention(\n",
            "          (linear_keys): Linear(in_features=512, out_features=512, bias=False)\n",
            "          (linear_values): Linear(in_features=512, out_features=512, bias=False)\n",
            "          (linear_query): Linear(in_features=512, out_features=512, bias=False)\n",
            "          (softmax): Softmax(dim=-1)\n",
            "          (dropout): Dropout(p=0.1, inplace=False)\n",
            "          (final_linear): Linear(in_features=512, out_features=512, bias=False)\n",
            "        )\n",
            "        (feed_forward): PositionwiseFeedForward(\n",
            "          (w_1): Linear(in_features=512, out_features=2048, bias=False)\n",
            "          (w_2): Linear(in_features=2048, out_features=512, bias=False)\n",
            "          (layer_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
            "          (dropout_1): Dropout(p=0.1, inplace=False)\n",
            "          (dropout_2): Dropout(p=0.1, inplace=False)\n",
            "        )\n",
            "        (layer_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
            "        (dropout): Dropout(p=0.1, inplace=False)\n",
            "      )\n",
            "    )\n",
            "    (layer_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
            "  )\n",
            "  (decoder): TransformerDecoder(\n",
            "    (embeddings): Embeddings(\n",
            "      (make_embedding): Sequential(\n",
            "        (emb_luts): Elementwise(\n",
            "          (0): Embedding(25000, 512, padding_idx=1)\n",
            "        )\n",
            "        (pe): PositionalEncoding()\n",
            "      )\n",
            "      (dropout): Dropout(p=0.1, inplace=False)\n",
            "    )\n",
            "    (layer_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
            "    (transformer_layers): ModuleList(\n",
            "      (0-5): 6 x TransformerDecoderLayer(\n",
            "        (self_attn): MultiHeadedAttention(\n",
            "          (linear_keys): Linear(in_features=512, out_features=512, bias=False)\n",
            "          (linear_values): Linear(in_features=512, out_features=512, bias=False)\n",
            "          (linear_query): Linear(in_features=512, out_features=512, bias=False)\n",
            "          (softmax): Softmax(dim=-1)\n",
            "          (dropout): Dropout(p=0.1, inplace=False)\n",
            "          (final_linear): Linear(in_features=512, out_features=512, bias=False)\n",
            "        )\n",
            "        (feed_forward): PositionwiseFeedForward(\n",
            "          (w_1): Linear(in_features=512, out_features=2048, bias=False)\n",
            "          (w_2): Linear(in_features=2048, out_features=512, bias=False)\n",
            "          (layer_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
            "          (dropout_1): Dropout(p=0.1, inplace=False)\n",
            "          (dropout_2): Dropout(p=0.1, inplace=False)\n",
            "        )\n",
            "        (layer_norm_1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
            "        (dropout): Dropout(p=0.1, inplace=False)\n",
            "        (context_attn): MultiHeadedAttention(\n",
            "          (linear_keys): Linear(in_features=512, out_features=512, bias=False)\n",
            "          (linear_values): Linear(in_features=512, out_features=512, bias=False)\n",
            "          (linear_query): Linear(in_features=512, out_features=512, bias=False)\n",
            "          (softmax): Softmax(dim=-1)\n",
            "          (dropout): Dropout(p=0.1, inplace=False)\n",
            "          (final_linear): Linear(in_features=512, out_features=512, bias=False)\n",
            "        )\n",
            "        (layer_norm_2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
            "      )\n",
            "    )\n",
            "  )\n",
            "  (generator): Linear(in_features=512, out_features=25000, bias=True)\n",
            ")\n",
            "[2024-09-30 03:27:09,397 INFO] encoder: 31683584\n",
            "[2024-09-30 03:27:09,397 INFO] decoder: 50810280\n",
            "[2024-09-30 03:27:09,397 INFO] * number of parameters: 82493864\n",
            "[2024-09-30 03:27:09,398 INFO] Trainable parameters = {'torch.float32': 82493864, 'torch.float16': 0, 'torch.uint8': 0, 'torch.int8': 0}\n",
            "[2024-09-30 03:27:09,398 INFO] Non trainable parameters = {'torch.float32': 0, 'torch.float16': 0, 'torch.uint8': 0, 'torch.int8': 0}\n",
            "[2024-09-30 03:27:09,398 INFO]  * src vocab size = 24992\n",
            "[2024-09-30 03:27:09,398 INFO]  * tgt vocab size = 25000\n",
            "[2024-09-30 03:27:09,708 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 1\n",
            "[2024-09-30 03:27:09,708 INFO] Starting training on GPU: [0]\n",
            "[2024-09-30 03:27:09,708 INFO] Start training loop and validate every 1000 steps...\n",
            "[2024-09-30 03:27:09,708 INFO] Scoring with: ['filtertoolong']\n",
            "[2024-09-30 03:27:12,541 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 2\n",
            "[2024-09-30 03:27:15,981 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 3\n",
            "[2024-09-30 03:27:18,263 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 4\n",
            "[2024-09-30 03:27:20,725 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 5\n",
            "[2024-09-30 03:28:38,213 INFO] Step 100/ 2000; acc: 7.3; ppl: 3852.4; xent: 8.3; lr: 0.00028; sents:   60867; bsz: 3603/3481/152; 16285/15734 tok/s;     89 sec;\n",
            "[2024-09-30 03:29:09,695 INFO] * Transform statistics for corpus_1(100.00%):\n",
            "\t\t\t* FilterTooLongStats(filtered=10)\n",
            "\n",
            "[2024-09-30 03:29:09,696 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 6\n",
            "[2024-09-30 03:29:12,682 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 7\n",
            "[2024-09-30 03:29:17,318 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 8\n",
            "[2024-09-30 03:29:19,120 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 9\n",
            "[2024-09-30 03:29:21,004 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 10\n",
            "[2024-09-30 03:30:15,874 INFO] Step 200/ 2000; acc: 20.8; ppl: 514.3; xent: 6.2; lr: 0.00056; sents:   59997; bsz: 3609/3497/150; 14783/14323 tok/s;    186 sec;\n",
            "[2024-09-30 03:31:20,113 INFO] Step 300/ 2000; acc: 29.9; ppl: 212.2; xent: 5.4; lr: 0.00084; sents:   59663; bsz: 3608/3464/149; 22467/21572 tok/s;    250 sec;\n",
            "[2024-09-30 03:31:22,180 INFO] * Transform statistics for corpus_1(100.00%):\n",
            "\t\t\t* FilterTooLongStats(filtered=10)\n",
            "\n",
            "[2024-09-30 03:31:22,180 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 11\n",
            "[2024-09-30 03:31:24,163 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 12\n",
            "[2024-09-30 03:31:25,859 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 13\n",
            "[2024-09-30 03:31:31,039 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 14\n",
            "[2024-09-30 03:31:32,814 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 15\n",
            "[2024-09-30 03:32:58,065 INFO] Step 400/ 2000; acc: 37.4; ppl: 109.5; xent: 4.7; lr: 0.00112; sents:   59816; bsz: 3595/3464/150; 14682/14147 tok/s;    348 sec;\n",
            "[2024-09-30 03:33:32,959 INFO] * Transform statistics for corpus_1(100.00%):\n",
            "\t\t\t* FilterTooLongStats(filtered=10)\n",
            "\n",
            "[2024-09-30 03:33:32,960 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 16\n",
            "[2024-09-30 03:33:34,836 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 17\n",
            "[2024-09-30 03:33:36,581 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 18\n",
            "[2024-09-30 03:33:42,077 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 19\n",
            "[2024-09-30 03:33:44,223 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 20\n",
            "[2024-09-30 03:34:36,284 INFO] Step 500/ 2000; acc: 45.0; ppl:  62.1; xent: 4.1; lr: 0.00140; sents:   60586; bsz: 3612/3476/151; 14712/14157 tok/s;    447 sec;\n",
            "[2024-09-30 03:35:40,584 INFO] Step 600/ 2000; acc: 50.6; ppl:  42.6; xent: 3.8; lr: 0.00168; sents:   60377; bsz: 3614/3497/151; 22481/21752 tok/s;    511 sec;\n",
            "[2024-09-30 03:35:44,494 INFO] * Transform statistics for corpus_1(100.00%):\n",
            "\t\t\t* FilterTooLongStats(filtered=10)\n",
            "\n",
            "[2024-09-30 03:35:44,495 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 21\n",
            "[2024-09-30 03:35:49,769 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 22\n",
            "[2024-09-30 03:35:51,476 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 23\n",
            "[2024-09-30 03:35:53,140 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 24\n",
            "[2024-09-30 03:35:59,496 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 25\n",
            "[2024-09-30 03:37:21,959 INFO] Step 700/ 2000; acc: 56.6; ppl:  28.9; xent: 3.4; lr: 0.00196; sents:   59486; bsz: 3604/3486/149; 14219/13753 tok/s;    612 sec;\n",
            "[2024-09-30 03:37:58,264 INFO] * Transform statistics for corpus_1(100.00%):\n",
            "\t\t\t* FilterTooLongStats(filtered=10)\n",
            "\n",
            "[2024-09-30 03:37:58,264 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 26\n",
            "[2024-09-30 03:38:00,053 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 27\n",
            "[2024-09-30 03:38:05,969 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 28\n",
            "[2024-09-30 03:38:07,673 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 29\n",
            "[2024-09-30 03:38:09,338 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 30\n",
            "[2024-09-30 03:38:59,345 INFO] Step 800/ 2000; acc: 61.4; ppl:  22.2; xent: 3.1; lr: 0.00224; sents:   61067; bsz: 3600/3505/153; 14785/14395 tok/s;    710 sec;\n",
            "[2024-09-30 03:40:03,085 INFO] Step 900/ 2000; acc: 62.5; ppl:  20.8; xent: 3.0; lr: 0.00252; sents:   59805; bsz: 3615/3453/150; 22685/21671 tok/s;    773 sec;\n",
            "[2024-09-30 03:40:08,841 INFO] * Transform statistics for corpus_1(100.00%):\n",
            "\t\t\t* FilterTooLongStats(filtered=10)\n",
            "\n",
            "[2024-09-30 03:40:08,841 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 31\n",
            "[2024-09-30 03:40:14,868 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 32\n",
            "[2024-09-30 03:40:16,602 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 33\n",
            "[2024-09-30 03:40:22,526 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 34\n",
            "[2024-09-30 03:40:24,222 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 35\n",
            "[2024-09-30 03:41:45,718 INFO] Step 1000/ 2000; acc: 67.3; ppl:  16.1; xent: 2.8; lr: 0.00279; sents:   59289; bsz: 3630/3488/148; 14149/13595 tok/s;    876 sec;\n",
            "[2024-09-30 03:41:47,006 INFO] valid stats calculation\n",
            "                           took: 1.2854511737823486 s.\n",
            "[2024-09-30 03:41:47,007 INFO] Train perplexity: 87.4097\n",
            "[2024-09-30 03:41:47,007 INFO] Train accuracy: 43.8934\n",
            "[2024-09-30 03:41:47,007 INFO] Sentences processed: 600953\n",
            "[2024-09-30 03:41:47,007 INFO] Average bsz: 3609/3481/150\n",
            "[2024-09-30 03:41:47,007 INFO] Validation perplexity: 65.8047\n",
            "[2024-09-30 03:41:47,007 INFO] Validation accuracy: 47.2197\n",
            "[2024-09-30 03:41:47,007 INFO] Model is improving ppl: inf --> 65.8047.\n",
            "[2024-09-30 03:41:47,007 INFO] Model is improving acc: -inf --> 47.2197.\n",
            "[2024-09-30 03:41:47,017 INFO] Saving checkpoint models/model.hien_step_1000.pt\n",
            "[2024-09-30 03:42:33,628 INFO] * Transform statistics for corpus_1(100.00%):\n",
            "\t\t\t* FilterTooLongStats(filtered=10)\n",
            "\n",
            "[2024-09-30 03:42:33,628 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 36\n",
            "[2024-09-30 03:42:35,917 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 37\n",
            "[2024-09-30 03:42:42,200 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 38\n",
            "[2024-09-30 03:42:43,883 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 39\n",
            "[2024-09-30 03:42:45,590 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 40\n",
            "[2024-09-30 03:43:33,301 INFO] Step 1100/ 2000; acc: 68.8; ppl:  15.0; xent: 2.7; lr: 0.00266; sents:   60854; bsz: 3570/3490/152; 13275/12975 tok/s;    984 sec;\n",
            "[2024-09-30 03:44:37,000 INFO] Step 1200/ 2000; acc: 71.6; ppl:  13.0; xent: 2.6; lr: 0.00255; sents:   60459; bsz: 3612/3459/151; 22684/21718 tok/s;   1047 sec;\n",
            "[2024-09-30 03:44:44,750 INFO] * Transform statistics for corpus_1(100.00%):\n",
            "\t\t\t* FilterTooLongStats(filtered=10)\n",
            "\n",
            "[2024-09-30 03:44:44,750 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 41\n",
            "[2024-09-30 03:44:50,888 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 42\n",
            "[2024-09-30 03:44:52,600 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 43\n",
            "[2024-09-30 03:44:58,712 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 44\n",
            "[2024-09-30 03:45:01,118 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 45\n",
            "[2024-09-30 03:46:19,765 INFO] Step 1300/ 2000; acc: 76.7; ppl:  10.4; xent: 2.3; lr: 0.00245; sents:   59996; bsz: 3637/3509/150; 14157/13657 tok/s;   1150 sec;\n",
            "[2024-09-30 03:46:59,791 INFO] * Transform statistics for corpus_1(100.00%):\n",
            "\t\t\t* FilterTooLongStats(filtered=10)\n",
            "\n",
            "[2024-09-30 03:46:59,792 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 46\n",
            "[2024-09-30 03:47:01,584 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 47\n",
            "[2024-09-30 03:47:03,310 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 48\n",
            "[2024-09-30 03:47:09,869 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 49\n",
            "[2024-09-30 03:47:11,722 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 50\n",
            "[2024-09-30 03:47:57,421 INFO] Step 1400/ 2000; acc: 78.7; ppl:   9.6; xent: 2.3; lr: 0.00236; sents:   60035; bsz: 3568/3435/150; 14616/14070 tok/s;   1248 sec;\n",
            "[2024-09-30 03:49:01,220 INFO] Step 1500/ 2000; acc: 80.6; ppl:   8.8; xent: 2.2; lr: 0.00228; sents:   60390; bsz: 3627/3500/151; 22739/21945 tok/s;   1312 sec;\n",
            "[2024-09-30 03:49:10,829 INFO] * Transform statistics for corpus_1(100.00%):\n",
            "\t\t\t* FilterTooLongStats(filtered=10)\n",
            "\n",
            "[2024-09-30 03:49:10,829 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 51\n",
            "[2024-09-30 03:49:16,139 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 52\n",
            "[2024-09-30 03:49:18,383 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 53\n",
            "[2024-09-30 03:49:20,916 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 54\n",
            "[2024-09-30 03:49:26,840 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 55\n",
            "[2024-09-30 03:50:42,789 INFO] Step 1600/ 2000; acc: 83.9; ppl:   7.7; xent: 2.0; lr: 0.00221; sents:   59758; bsz: 3584/3484/149; 14114/13722 tok/s;   1413 sec;\n",
            "[2024-09-30 03:51:25,021 INFO] * Transform statistics for corpus_1(100.00%):\n",
            "\t\t\t* FilterTooLongStats(filtered=10)\n",
            "\n",
            "[2024-09-30 03:51:25,022 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 56\n",
            "[2024-09-30 03:51:26,865 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 57\n",
            "[2024-09-30 03:51:29,814 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 58\n",
            "[2024-09-30 03:51:35,356 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 59\n",
            "[2024-09-30 03:51:37,060 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 60\n",
            "[2024-09-30 03:52:20,763 INFO] Step 1700/ 2000; acc: 84.1; ppl:   7.6; xent: 2.0; lr: 0.00214; sents:   60291; bsz: 3630/3484/151; 14822/14225 tok/s;   1511 sec;\n",
            "[2024-09-30 03:53:24,497 INFO] Step 1800/ 2000; acc: 86.5; ppl:   6.9; xent: 1.9; lr: 0.00208; sents:   60532; bsz: 3597/3489/151; 22575/21896 tok/s;   1575 sec;\n",
            "[2024-09-30 03:53:36,061 INFO] * Transform statistics for corpus_1(100.00%):\n",
            "\t\t\t* FilterTooLongStats(filtered=10)\n",
            "\n",
            "[2024-09-30 03:53:36,061 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 61\n",
            "[2024-09-30 03:53:38,945 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 62\n",
            "[2024-09-30 03:53:44,221 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 63\n",
            "[2024-09-30 03:53:45,938 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 64\n",
            "[2024-09-30 03:53:52,402 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 65\n",
            "[2024-09-30 03:55:04,693 INFO] Step 1900/ 2000; acc: 87.3; ppl:   6.7; xent: 1.9; lr: 0.00203; sents:   59943; bsz: 3580/3441/150; 14292/13735 tok/s;   1675 sec;\n",
            "[2024-09-30 03:55:48,969 INFO] * Transform statistics for corpus_1(100.00%):\n",
            "\t\t\t* FilterTooLongStats(filtered=10)\n",
            "\n",
            "[2024-09-30 03:55:48,970 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 66\n",
            "[2024-09-30 03:55:50,782 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 67\n",
            "[2024-09-30 03:55:56,174 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 68\n",
            "[2024-09-30 03:55:57,961 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 69\n",
            "[2024-09-30 03:56:05,330 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 70\n",
            "[2024-09-30 03:56:45,342 INFO] Step 2000/ 2000; acc: 88.5; ppl:   6.4; xent: 1.9; lr: 0.00198; sents:   60096; bsz: 3621/3519/150; 14392/13986 tok/s;   1776 sec;\n",
            "[2024-09-30 03:56:45,412 INFO] * Transform statistics for valid(100.00%):\n",
            "\t\t\t* FilterTooLongStats(filtered=1)\n",
            "\n",
            "[2024-09-30 03:56:46,601 INFO] valid stats calculation\n",
            "                           took: 1.2572627067565918 s.\n",
            "[2024-09-30 03:56:46,602 INFO] Train perplexity: 27.8505\n",
            "[2024-09-30 03:56:46,602 INFO] Train accuracy: 62.2907\n",
            "[2024-09-30 03:56:46,602 INFO] Sentences processed: 1.20331e+06\n",
            "[2024-09-30 03:56:46,602 INFO] Average bsz: 3606/3481/150\n",
            "[2024-09-30 03:56:46,602 INFO] Validation perplexity: 63.4597\n",
            "[2024-09-30 03:56:46,602 INFO] Validation accuracy: 51.6055\n",
            "[2024-09-30 03:56:46,602 INFO] Model is improving ppl: 65.8047 --> 63.4597.\n",
            "[2024-09-30 03:56:46,602 INFO] Model is improving acc: 47.2197 --> 51.6055.\n",
            "[2024-09-30 03:56:46,612 INFO] Saving checkpoint models/model.hien_step_2000.pt\n"
          ]
        }
      ],
      "source": [
        "# Training the Nueral Machine Translation Model\n",
        "!onmt_train -config config.yaml"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r8uFCS8Tez1k",
        "outputId": "c5e29028-f03e-4bc4-dfb8-e41adf4c7d3b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2024-09-30 03:57:06,863 INFO] Loading checkpoint from models/model.hien_step_1000.pt\n",
            "[2024-09-30 03:57:08,072 INFO] Loading data into the model\n",
            "[2024-09-30 03:57:34,095 INFO] PRED SCORE: -0.6578, PRED PPL: 1.93 NB SENTENCES: 2000\n",
            "Time w/o python interpreter load/terminate:  27.24466323852539\n"
          ]
        }
      ],
      "source": [
        "# Translation work...\n",
        "!onmt_translate -model models/model.hien_step_1000.pt -src pmindia.en-hi.hi-filtered.hi.subword.test -output pmindia.en-hi.en.translated -gpu 0 -min_length 1\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kaJ2Jy5Om9f0",
        "outputId": "ada09e10-712c-4068-fc7e-a0dd810ff215"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "▁The ▁Prim e ▁Minister ▁met ▁the ▁Secretary ▁General ▁Ban ▁Ki ▁Moon ▁on ▁the ▁Unit ed ▁Nations ▁General ▁Ban ▁Ki ▁Moon .\n",
            "▁The ▁delegation ▁members ▁of ▁the ▁Unit ed ▁Nations ▁Dr . ▁S . P . J . ▁Abdul ▁Kalam , ▁and ▁National ▁Security ▁Adviser ▁Ajit ▁Doval , ▁held ▁in ▁July ▁ 2 0 1 5 .\n",
            "▁Terrorism ▁is ▁a ▁new ▁day ▁and ▁name .\n",
            "▁There ▁is ▁tremendous ▁scope ▁for ▁growth ▁in ▁towns ▁and ▁cities .\n",
            "▁ 5 9 . ▁The ▁Sides ▁committed ▁to ▁strengthening ▁the ▁global ▁non - proliferation ▁of ▁nuclear ▁disarmament ▁and ▁its ▁commitment .\n"
          ]
        }
      ],
      "source": [
        "!head -n 5 pmindia.en-hi.en.translated"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "1RVesqp2nbME"
      },
      "outputs": [],
      "source": [
        "!pip install --upgrade -q sentencepiece"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fl1klUVdoFMr",
        "outputId": "2ead3469-69c4-46df-f421-ceeac6c8d5f8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Done desubwording! Output: pmindia.en-hi.en.translated.desubword\n"
          ]
        }
      ],
      "source": [
        "# Working on desubwording the translation file produced\n",
        "!python3 MT-Preparation/subwording/3-desubword.py target.model pmindia.en-hi.en.translated"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "WbGOc4H7o_za",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bed994bf-f70e-4af7-e971-206d1b26cb60"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The Prime Minister met the Secretary General Ban Ki Moon on the United Nations General Ban Ki Moon.\n",
            "The delegation members of the United Nations Dr. S.P.J. Abdul Kalam, and National Security Adviser Ajit Doval, held in July 2015.\n",
            "Terrorism is a new day and name.\n",
            "There is tremendous scope for growth in towns and cities.\n",
            "59. The Sides committed to strengthening the global non-proliferation of nuclear disarmament and its commitment.\n"
          ]
        }
      ],
      "source": [
        "!head -n 5 pmindia.en-hi.en.translated.desubword"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UL1ajQaZpa1E",
        "outputId": "d730ac81-71f0-43eb-83e0-ef6cbcddcfa1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2024-09-30 03:57:38--  https://raw.githubusercontent.com/ymoslem/MT-Evaluation/main/BLEU/compute-bleu.py\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 957 [text/plain]\n",
            "Saving to: ‘compute-bleu.py’\n",
            "\n",
            "compute-bleu.py     100%[===================>]     957  --.-KB/s    in 0s      \n",
            "\n",
            "2024-09-30 03:57:38 (71.2 MB/s) - ‘compute-bleu.py’ saved [957/957]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Machine Translation evaluation\n",
        "!wget https://raw.githubusercontent.com/ymoslem/MT-Evaluation/main/BLEU/compute-bleu.py"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Xz4Zz5X8pooL",
        "outputId": "b012a3a5-5ab1-4a68-e0f2-6b25ca0ca0c8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: sacrebleu in /usr/local/lib/python3.10/dist-packages (2.4.3)\n",
            "Requirement already satisfied: portalocker in /usr/local/lib/python3.10/dist-packages (from sacrebleu) (2.10.1)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.10/dist-packages (from sacrebleu) (2024.9.11)\n",
            "Requirement already satisfied: tabulate>=0.8.9 in /usr/local/lib/python3.10/dist-packages (from sacrebleu) (0.9.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from sacrebleu) (1.26.4)\n",
            "Requirement already satisfied: colorama in /usr/local/lib/python3.10/dist-packages (from sacrebleu) (0.4.6)\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.10/dist-packages (from sacrebleu) (4.9.4)\n"
          ]
        }
      ],
      "source": [
        "# Installing sacrebleu\n",
        "!pip3 install sacrebleu"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NK9JjTDYqSBT",
        "outputId": "bb0f720d-91bc-48e7-9215-675c6181f0cc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "compute-bleu.py\t\t\t\t    pmindia.en-hi.hi\n",
            "config.yaml\t\t\t\t    pmindia.en-hi.hi-filtered.hi\n",
            "en-hi.txt.zip\t\t\t\t    pmindia.en-hi.hi-filtered.hi.subword\n",
            "LICENSE\t\t\t\t\t    pmindia.en-hi.hi-filtered.hi.subword.dev\n",
            "models\t\t\t\t\t    pmindia.en-hi.hi-filtered.hi.subword.test\n",
            "MT-Preparation\t\t\t\t    pmindia.en-hi.hi-filtered.hi.subword.train\n",
            "pmindia.en-hi.en\t\t\t    pmindia.en-hi.xml\n",
            "pmindia.en-hi.en-filtered.en\t\t    README\n",
            "pmindia.en-hi.en-filtered.en.subword\t    run\n",
            "pmindia.en-hi.en-filtered.en.subword.dev    source.model\n",
            "pmindia.en-hi.en-filtered.en.subword.test   source.vocab\n",
            "pmindia.en-hi.en-filtered.en.subword.train  target.model\n",
            "pmindia.en-hi.en.translated\t\t    target.vocab\n",
            "pmindia.en-hi.en.translated.desubword\t    train.log\n"
          ]
        }
      ],
      "source": [
        "!ls"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SHFARahcqX1z",
        "outputId": "6fd66dea-8b10-4aca-e8bd-2af50acffc30"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "BLEU score: 0.6435767709192263\n"
          ]
        }
      ],
      "source": [
        "import sacrebleu\n",
        "\n",
        "# Reference Translation\n",
        "with open(\"pmindia.en-hi.en-filtered.en\", \"r\", encoding=\"utf-8\") as ref_file:\n",
        "    references = [line.strip() for line in ref_file]\n",
        "\n",
        "# Model Translation\n",
        "with open(\"pmindia.en-hi.en.translated.desubword\", \"r\", encoding=\"utf-8\") as pred_file:\n",
        "    predictions = [line.strip() for line in pred_file]\n",
        "\n",
        "# BLEU score\n",
        "bleu = sacrebleu.corpus_bleu(predictions, [references])\n",
        "print(f\"BLEU score: {bleu.score}\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "TMBUmu-cIbkl"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}